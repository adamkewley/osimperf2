#!/usr/bin/env python3
import csv
import glob
import pprint
import tomllib
from concurrent.futures import ThreadPoolExecutor
from datetime import date, timedelta, datetime, timezone, tzinfo
from pathlib import Path
from tempfile import TemporaryDirectory
import argparse
import logging
import os
import shlex
import shutil
import subprocess
import random

def _load_broken_commits():
    broken_commits_path = "broken-commits.txt"

    # Load broken-commits.txt into a set
    return {
        line.strip()
        for line in Path(broken_commits_path).read_text().splitlines()
        if line.strip() and not line.startswith("#")
    }

class Config:
    def __init__(self):
        self.log_level = logging.INFO
        self.source_path = Path("opensim-core")
        self.builds_path = Path("builds")
        self.installs_path = Path("opensim-core_installs")
        self.runner_source_path = Path("osimperf2-runner.cpp")
        self.build_type = "Release"
        self.ignored_commits = _load_broken_commits()

# Runs a "--run" type of benchmark
class RunBenchmarkRunner:
    @staticmethod
    def load_from_dict(benchmark_directory, d):
        assert d["type"] == "run", "only 'run' benchmarks currently supported"

        name = benchmark_directory.name
        model_path = (benchmark_directory / d["run"]["model"])
        end_time = d["run"]["end_time"]

        return RunBenchmarkRunner(name, model_path, end_time)

    def __init__(self, name, model_path : Path, end_time : float):
        assert model_path.exists()
        assert end_time > 0

        self.name = name
        self.model_path = model_path
        self.end_time = end_time

    def __repr__(self):
        return pprint.pformat(vars(self))

    def run(self, osimperf2_exe_path : Path):
        with TemporaryDirectory() as td:
            logging.info("Workflow directory %s", td)
            result_file = Path(td) / "result.txt"
            args = [
                str(osimperf2_exe_path.resolve()),
                "--model",
                str(self.model_path),
                "--out",
                str(result_file.resolve()),
                "--run",
                str(self.end_time)
            ]

            # Add the `lib/` to `LD_LIBRARY_PATH`, because some builds of
            # OpenSim do not handle Lepton linkage correctly
            env = os.environ.copy()
            env["LD_LIBRARY_PATH"] = f"{osimperf2_exe_path.parent.parent / 'lib'}:{env.get('LD_LIBRARY_PATH', '')}"
            out = subprocess.run(args, capture_output=True, env=env)
            if out.returncode != 0:
                logging.error(f"Error running {' '.join(shlex.quote(a) for a in args)}")
                logging.error("stdout: %s", out.stderr)
                logging.error("stderr: %s", out.stderr)
                raise RuntimeError("Error running measurement")

            return float(result_file.read_text())

class CommitDateLut:
    def __init__(self):
        self._lut = {}

    def lookup(self, config : Config, commit):
        if commit in self._lut:
            return self._lut[commit]
        else:
            unix_timestamp_str = subprocess.check_output(["git", "-C", config.source_path, "show", "-s", "--format=%ct", commit]).strip()
            unix_timestamp = int(unix_timestamp_str)
            return datetime.fromtimestamp(unix_timestamp, tz=timezone.utc)

class RunnableCommit:
    def __init__(self, commit_time : datetime, commit : str, runner_path : Path):
        self.commit_time = commit_time
        self.commit = commit
        self.runner_path = runner_path

    def __repr__(self):
        return pprint.pformat(vars(self))

class AnalysisMeasurement:

    def __init__(
            self,
            measurement_time : datetime,
            commit : str,
            commit_time : datetime,
            benchmark_name : str,
            repeat_number : int,
            runtime : float):
        self.measurement_time = measurement_time
        self.commit = commit
        self.commit_time = commit_time
        self.benchmark_name = benchmark_name
        self.repeat_number = repeat_number
        self.runtime = runtime

    def __repr__(self):
        return pprint.pformat(vars(self))

    def to_string_dict(self):
        # measurement_time,commit,commit_time,benchmark_name,repeat_number, runtime
        return {
            "measurement_time": str(int(self.measurement_time.timestamp())),
            "commit": self.commit,
            "commit_time": str(int(self.commit_time.timestamp())),
            "benchmark_name": self.benchmark_name,
            "repeat_number": str(self.repeat_number),
            "runtime": str(self.runtime),
        }

class AnalysisPlannedMeasurement:

    def __init__(
            self,
            commit : RunnableCommit,
            benchmark_runner : RunBenchmarkRunner,
            repeat : int):
        self.commit = commit
        self.benchmark_runner = benchmark_runner
        self.run_id = repeat

    def __repr__(self):
        return pprint.pformat(vars(self))

    def run(self):
        measurement_time = datetime.now(timezone.utc)
        dt = self.benchmark_runner.run(self.commit.runner_path)
        return AnalysisMeasurement(
            measurement_time,
            self.commit.commit,
            self.commit.commit_time,
            self.benchmark_runner.name,
            self.run_id,
            dt
        )

def _load_runner_from_benchmark_config(benchmark_config_toml):
    with open(benchmark_config_toml, "rb") as f:
        benchmark_config = tomllib.load(f)
    assert benchmark_config["type"] == "run", "only 'run' benchmarks currently supported"
    return RunBenchmarkRunner.load_from_dict(benchmark_config_toml.parent, benchmark_config)

def _date_sample_grid(start, end, mode="monthly"):
    mode_months = {"annually": [1], "quaterly": list(range(1, 13, 3)), "monthly": list(range(1, 13))}

    dates = []
    for year in range(start.year, end.year + 1):
        for month in mode_months[mode]:
            d = date(year, month, 1)
            if d <= end:  # donâ€™t go beyond current date
                dates.append(d)
    return dates

def _run(args):
    cmd_str = " ".join(shlex.quote(a) for a in args)
    logging.info("running %s", cmd_str)
    return subprocess.run(args, check=True)

def _commit_for(config, date):
    assert config.source_path.exists(), f"{config.source_path}: does not exist, cannot read git history"
    proc = subprocess.run(["git", "-C", str(config.source_path), "rev-list", "-10", f"--before={date}T09:00:00Z", "main"], check=True, capture_output=True, text=True)
    commits = [l for line in proc.stdout.splitlines() if (l := line.strip()) not in config.ignored_commits]
    if not commits:
        raise RuntimeError(f"No suitable commit found  before {date}: all nearby commits are broken")
    return commits[0]

def _build_runner_for(config, commit):
    commit_build_path = config.builds_path / commit
    commit_install_path = config.installs_path / commit
    runner_path = commit_install_path / "bin" / "osimperf2-runner"

    build_runner = False
    if not runner_path.exists():
        logging.info("%s does not exist: building", runner_path)
        build_runner = True
    elif runner_path.stat().st_mtime < config.runner_source_path.stat().st_mtime:
        logging.info("%s is older than %s: rebuilding", runner_path, config.runner_source_path)
        build_runner = True

    if build_runner:
        runner_build_path = commit_build_path / "runner-build"
        _run([
            "cmake",
            "-G", "Ninja",
            "-S", ".",
            "-B", str(runner_build_path.resolve()),
            f"-DCMAKE_PREFIX_PATH={commit_install_path.resolve()}",
            f"-DCMAKE_INSTALL_PREFIX={commit_install_path.resolve()}",
            f"-DCMAKE_BUILD_TYPE={config.build_type}"
        ])
        _run([
            "cmake",
            "--build", str(runner_build_path.resolve()),
            "--target", "install"
        ])
        logging.info("cleaning up build dir %s", runner_build_path)
        shutil.rmtree(runner_build_path)
        logging.info("built runner at %s", runner_path)

def _build_runners_for(config, commits):
    # The reason this is separate from the general "build opensim" step
    # is because the runners are more likely to change in-tree but are
    # only one source file, so can be parallelized more easily.
    with ThreadPoolExecutor() as executor:
        task_iterator = executor.map(_build_runner_for, [config for _ in commits], commits)
        list(task_iterator)

def _build_commit(config, commit): 
    commit_build_path = config.builds_path / commit
    commit_install_path = config.installs_path / commit
    runner_path = commit_install_path / "bin" / "osimperf2-runner"

    assert os.path.commonpath([commit_build_path.resolve(), commit_install_path.resolve()]) != str(commit_build_path.resolve())

    # Check, configure, build, and install the opensim-core API (if necessary)
    if not commit_install_path.exists():
        # Ensure builds/ directory exists
        config.builds_path.mkdir(parents=True, exist_ok=True)

        # Checkout commit
        logging.info("%s requires building, building it", commit)
        logging.info("checking out %s", commit)
        _run(["git", "-C", str(config.source_path), "checkout", commit])

        # Configure + build dependencies
        dependency_build_path = commit_build_path / "dependencies-build"
        dependency_install_path = commit_build_path / "dependencies-install"
        logging.info("building %s's dependencies to %s", commit, dependency_build_path)
        _run([
            "cmake",
            "-S", str(config.source_path / "dependencies"),
            "-B", str(dependency_build_path.resolve()),
            "-G", "Ninja",
            f"-DCMAKE_INSTALL_PREFIX={dependency_install_path.resolve()}",
            f"-DCMAKE_BUILD_TYPE={config.build_type}",
            "-DOPENSIM_WITH_CASADI=OFF",
            "-DOPENSIM_WITH_TROPTER=OFF",
            "-DSUPERBUILD_docopt=OFF",
            "-DSUPERBUILD_BTK=OFF",
            "-DSUPERBUILD_ipopt=OFF",
            "-DSUPERBUILD_adolc=OFF",
            "-DSUPERBUILD_casadi=OFF",
        ])
        _run([
            "cmake",
            "--build", str(dependency_build_path.resolve())
        ])

        # Configure + build + install OpenSim
        main_build_path = commit_build_path / "main-build"
        logging.info("building %s in %s", commit, main_build_path)
        _run([
            "cmake",
            "-S", str(config.source_path),
            "-B", str(main_build_path.resolve()),
            "-G", "Ninja",
            f"-DCMAKE_INSTALL_PREFIX={commit_install_path.resolve()}",
            "-DBUILD_JAVA_WRAPPING=OFF",
            "-DBUILD_PYTHON_WRAPPING=OFF",
            f"-DCMAKE_BUILD_TYPE={config.build_type}",
            "-DOPENSIM_DISABLE_LOG_FILE=ON",
            "-DOPENSIM_WITH_CASADI=OFF",
            "-DOPENSIM_WITH_TROPTER=OFF",
            "-DOPENSIM_COPY_DEPENDENCIES=ON",
            f"-DOPENSIM_DEPENDENCIES_DIR={dependency_install_path.resolve()}",
            "-DBUILD_API_ONLY=ON",
            "-DBUILD_TESTING=OFF",
            "-DBUILD_API_EXAMPLES=OFF",
            "-DOPENSIM_WITH_MOCO=OFF",
        ])
        _run([
            "cmake",
            "--build", str(main_build_path.resolve()),
            "--target", "install"
        ])

        # Create bin/ install directory (some legacy opensim-core cmake files break otherwise)
        (commit_install_path / "bin").touch()

        # Patch headers that contain stuff that isn't compatible with modern C++
        broken_header = commit_install_path / "include/OpenSim/Common/PropertySet.h"
        if broken_header.exists():
            content = broken_header.read_text()
            patched_content = content.replace(") throw (Exception)", ")")
            if patched_content != content:
                logging.info(f"patching checked exceptions out of {broken_header}")
                broken_header.write_text(patched_content)

        # Clean out build directory (it's installed)
        logging.info("cleaning out %s", commit_build_path)
        shutil.rmtree(commit_build_path)
    else:
        logging.info("%s opensim-core is already built, skipping", commit)

def _all_sample_dates(config):
    end = date.today()
    start = date(end.year - 8, end.month, end.day)
    return _date_sample_grid(start, end)

def all_commits(config):
    for date in _all_sample_dates(config):
        commit = _commit_for(config, date)
        print(commit)

def build_all(config):
    commits = []
    for d in _all_sample_dates(config):
        commit = _commit_for(config, d)
        _build_commit(config, commit)
        commits.append(commit)
    _build_runners_for(config, commits)

def build(config, commit):
    _build_commit(config, commit)
    _build_runners_for(config, [commit])

def measure(config, commit, benchmark_name):
    runner_path = config.installs_path / commit / "bin" / "osimperf2-runner"
    assert runner_path.exists(), f"{runner_path}: does not exist, has `osimperf2-runner` been built for this commit?"

    benchmark_config_path = Path("benchmarks") / benchmark_name / "benchmark_config.toml"
    assert benchmark_config_path.exists()

    runner = _load_runner_from_benchmark_config(benchmark_config_path)
    dt = runner.run(runner_path)
    print(dt)

def update_measurements(config, analysis_toml):
    # Load analysis configuration toml file
    with open(analysis_toml, "rb") as f:
        analysis_config = tomllib.load(f)

    validation_errors = []

    commit_to_date_lut = CommitDateLut()  # accelerates date lookups

    # Collect + validate `commits`
    commits = []
    for commit_config_entry in analysis_config["commits"]:
        for expanded_entry in glob.glob(commit_config_entry):
            commit_hash = Path(expanded_entry).name
            commit_date = commit_to_date_lut.lookup(config, commit_hash)
            commit_runner_path = (Path(expanded_entry) / "bin/osimperf2-runner").resolve()
            if commit_runner_path.exists():
                commits.append(RunnableCommit(commit_date, commit_hash, commit_runner_path))
            else:
                validation_errors.append(f"commits: {commit_runner_path}: does not exist. Maybe you need to run `build-all`?")
    logging.info("%s commits will be analyzed", len(commits))

    # Collect + validate `benchmarks`
    benchmark_runners = []
    for benchmark_config_entry in analysis_config["benchmarks"]:
        for expanded_entry in glob.glob(benchmark_config_entry):
            benchmark_config_path = Path(expanded_entry) / "benchmark_config.toml"
            if benchmark_config_path.exists():
                benchmark_runners.append(_load_runner_from_benchmark_config(benchmark_config_path))
            else:
                validation_errors.append(f"benchmarks: {benchmark_config_path}: does not exist. Add a benchmark configuration file.")
    logging.info("%s benchmarks will be analyzed", len(benchmark_runners))

    # Read + validate `repeats`
    repeats = 1
    if isinstance(analysis_config["repeats"], int) and analysis_config["repeats"] > 0:
        repeats = analysis_config["repeats"]
    else:
        validation_errors.append("repeats: is not a valid configuration value, must be a positive integer")
    logging.info("%s repeats of each (commit, benchmark) combination will be measured", repeats)

    # Read + validate `result_csv`
    result_csv_path = Path(analysis_config["result_csv"])
    existing_measurements = set()
    if result_csv_path.exists():
        logging.info("%s already exists, the analysis will skip existing measurements and only append new ones", result_csv_path)
        with open(result_csv_path, newline='', encoding='utf-8') as result_csv:
            reader = csv.DictReader(result_csv)
            for row in reader:
                assert False, "TODO parse it into a sequence of existing measurements"

    # Exit if there's any validation errors in the analysis configuration
    if validation_errors:
        message = f"Validation errors found in {analysis_toml}:\n"
        for validation_error in validation_errors:
            message += f"  - {validation_error}\n"
        raise RuntimeError(message)

    # Calculate complete sequence of measurements that could be taken
    measurements_to_take = []
    for commit in commits:
        for benchmark_runner in benchmark_runners:
            for repeat in range(0, repeats):
                measurements_to_take.append(AnalysisPlannedMeasurement(commit, benchmark_runner, repeat))

    csv_fieldnames = ["measurement_time", "commit", "commit_time", "benchmark_name", "repeat_number", "runtime"]

    # Subtract existing results
    if existing_measurements:
        result_csv_path.unlink()
        logging.warning("TODO: subtract existing results from the measurement set")

    if not result_csv_path.exists():
        # Write CSV header
        with open(result_csv_path, "wt") as f:
            writer = csv.DictWriter(f, fieldnames=csv_fieldnames)
            writer.writeheader()

    # Shuffle measurements that should be taken
    random.shuffle(measurements_to_take)

    csv_fieldnames = ["measurement_time", "commit", "commit_time", "benchmark_name", "repeat_number", "runtime"]

    for measurement_to_take in measurements_to_take:
        measurement = measurement_to_take.run()
        measurement_dict = measurement.to_string_dict()

        logging.info("measurement result = %s", measurement_dict)

        # Append measurement to result file
        with open(result_csv_path, "a", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=csv_fieldnames)
            writer.writerow(measurement_dict)

def main():
    config = Config()
    logging.basicConfig(level=config.log_level)

    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest="command", required=True)

    subparsers.add_parser("all-commits")          # no args
    subparsers.add_parser("build-all")            # no args
    
    full_analysis_parser = subparsers.add_parser("full-analysis")
    full_analysis_parser.add_argument("measurements_toml")

    build_parser = subparsers.add_parser("build")
    build_parser.add_argument("commit")

    measure_parser = subparsers.add_parser("measure")
    measure_parser.add_argument("commit")
    measure_parser.add_argument("benchmark_name")

    args = parser.parse_args()
    if args.command == "all-commits":
        all_commits(config)
    elif args.command == "build-all":
        build_all(config)
    elif args.command == "full-analysis":
        update_measurements(config, args.measurements_toml)
    elif args.command == "build":
        build(config, args.commit)
    elif args.command == "measure":
        measure(config, args.commit, args.benchmark_name)
    else:
        raise RuntimeError("unhandled command detected: probably a developer's fault!")

if __name__ == "__main__":
    main()
