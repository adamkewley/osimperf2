#!/usr/bin/env python3

import csv
import glob
import pprint
import tomllib
from concurrent.futures import ThreadPoolExecutor
from datetime import date, timedelta, datetime, timezone, tzinfo
from pathlib import Path
from tempfile import TemporaryDirectory
import argparse
import logging
import os
import shlex
import shutil
import subprocess
import random

import pandas as pd
import matplotlib.pyplot as plt

# valgrind --tool=callgrind --dump-instr=yes --collect-jumps=yes --cache-sim=yes --branch-sim=yes --callgrind-out-file="$WORKSPACE/RajagopalDrop-grind.out"

def _load_broken_commits():
    broken_commits_path = "broken-commits.txt"

    # Load broken-commits.txt into a set
    return {
        line.strip()
        for line in Path(broken_commits_path).read_text().splitlines()
        if line.strip() and not line.startswith("#")
    }

class Config:
    def __init__(self):
        self.log_level = logging.INFO
        self.source_path = Path("opensim-core")
        self.builds_path = Path("builds")
        self.installs_path = Path("opensim-core_installs")
        self.runner_source_path = Path("osimperf2-runner.cpp")
        self.build_type = "Release"
        self.ignored_commits = _load_broken_commits()

# Runs a "--run" type of benchmark
class RunBenchmarkRunner:
    @staticmethod
    def load_from_dict(benchmark_directory, d):
        assert d["type"] == "run", "only 'run' benchmarks currently supported"

        name = benchmark_directory.name
        model_path = (benchmark_directory / d["run"]["model"])
        end_time = d["run"]["end_time"]

        return RunBenchmarkRunner(name, model_path, end_time)

    def __init__(self, name, model_path : Path, end_time : float):
        assert model_path.exists()
        assert end_time > 0

        self.name = name
        self.model_path = model_path
        self.end_time = end_time

    def __repr__(self):
        return pprint.pformat(vars(self))

    def run(self, osimperf2_exe_path : Path):
        with TemporaryDirectory() as td:
            logging.info("Workflow directory %s", td)
            result_file = Path(td) / "result.txt"
            args = [
                str(osimperf2_exe_path.resolve()),
                "--model",
                str(self.model_path),
                "--out",
                str(result_file.resolve()),
                "--run",
                str(self.end_time)
            ]

            # Add the `lib/` to `LD_LIBRARY_PATH`, because some builds of
            # OpenSim do not handle Lepton linkage correctly
            env = os.environ.copy()
            env["LD_LIBRARY_PATH"] = f"{osimperf2_exe_path.parent.parent / 'lib'}:{env.get('LD_LIBRARY_PATH', '')}"
            out = subprocess.run(args, capture_output=True, env=env)
            if out.returncode != 0:
                logging.error(f"Error running {' '.join(shlex.quote(a) for a in args)}")
                logging.error("stdout: %s", out.stderr)
                logging.error("stderr: %s", out.stderr)
                raise RuntimeError("Error running measurement")

            return float(result_file.read_text())

class CommitDateLut:
    def __init__(self):
        self._lut = {}

    def lookup(self, config : Config, commit):
        if commit in self._lut:
            return self._lut[commit]
        else:
            unix_timestamp_str = subprocess.check_output(["git", "-C", config.source_path, "show", "-s", "--format=%ct", commit]).strip()
            unix_timestamp = int(unix_timestamp_str)
            return datetime.fromtimestamp(unix_timestamp, tz=timezone.utc)

class RunnableCommit:
    def __init__(self, commit_time : datetime, commit : str, runner_path : Path):
        self.commit_time = commit_time
        self.commit = commit
        self.runner_path = runner_path

    def __repr__(self):
        return pprint.pformat(vars(self))

class AnalysisMeasurement:

    def __init__(
            self,
            measurement_time : datetime,
            commit : str,
            commit_time : datetime,
            benchmark_name : str,
            repeat_number : int,
            runtime : float):
        self.measurement_time = measurement_time
        self.commit = commit
        self.commit_time = commit_time
        self.benchmark_name = benchmark_name
        self.repeat_number = repeat_number
        self.runtime = runtime

    def __repr__(self):
        return pprint.pformat(vars(self))

    def to_string_dict(self):
        # measurement_time,commit,commit_time,benchmark_name,repeat_number, runtime
        return {
            "measurement_time": str(int(self.measurement_time.timestamp())),
            "commit": self.commit,
            "commit_time": str(int(self.commit_time.timestamp())),
            "benchmark_name": self.benchmark_name,
            "repeat_number": str(self.repeat_number),
            "runtime": str(self.runtime),
        }

class AnalysisPlannedMeasurement:

    def __init__(
            self,
            commit : RunnableCommit,
            benchmark_runner : RunBenchmarkRunner,
            repeat : int):
        self.commit = commit
        self.benchmark_runner = benchmark_runner
        self.run_id = repeat

    def __repr__(self):
        return pprint.pformat(vars(self))

    def __eq__(self, rhs):
        if isinstance(rhs, AnalysisPlannedMeasurement):
            return (self.commit.commit, self.benchmark_runner.name, self.run_id) == (rhs.commit.commit, rhs.benchmark_runner.name, rhs.run_id)
        return NotImplemented

    def __hash__(self):
        return hash((self.commit.commit, self.benchmark_runner.name, self.run_id))

    def to_string_dict(self):
        return {
            "commit": self.commit.commit,
            "benchmark_name": self.benchmark_runner.name,
            "repeat_number": self.run_id,
        }

    def run(self):
        measurement_time = datetime.now(timezone.utc)
        dt = self.benchmark_runner.run(self.commit.runner_path)
        return AnalysisMeasurement(
            measurement_time,
            self.commit.commit,
            self.commit.commit_time,
            self.benchmark_runner.name,
            self.run_id,
            dt
        )

def _load_runner_from_benchmark_config(benchmark_config_toml):
    with open(benchmark_config_toml, "rb") as f:
        benchmark_config = tomllib.load(f)
    assert benchmark_config["type"] == "run", "only 'run' benchmarks currently supported"
    return RunBenchmarkRunner.load_from_dict(benchmark_config_toml.parent, benchmark_config)

def _date_sample_grid(start, end, mode="monthly"):
    mode_months = {"annually": [1], "quaterly": list(range(1, 13, 3)), "monthly": list(range(1, 13))}

    dates = []
    for year in range(start.year, end.year + 1):
        for month in mode_months[mode]:
            d = date(year, month, 1)
            if d <= end:  # donâ€™t go beyond current date
                dates.append(d)
    return dates

def _run(args):
    cmd_str = " ".join(shlex.quote(a) for a in args)
    logging.info("running %s", cmd_str)
    return subprocess.run(args, check=True)

def _commit_for(config, date):
    assert config.source_path.exists(), f"{config.source_path}: does not exist, cannot read git history"
    proc = subprocess.run(["git", "-C", str(config.source_path), "rev-list", "-10", f"--before={date}T09:00:00Z", "main"], check=True, capture_output=True, text=True)
    commits = [l for line in proc.stdout.splitlines() if (l := line.strip()) not in config.ignored_commits]
    if not commits:
        raise RuntimeError(f"No suitable commit found  before {date}: all nearby commits are broken")
    return commits[0]

def _build_runner_for(config, commit):
    commit_build_path = config.builds_path / commit
    commit_install_path = config.installs_path / commit
    runner_path = commit_install_path / "bin" / "osimperf2-runner"

    build_runner = False
    if not runner_path.exists():
        logging.info("%s does not exist: building", runner_path)
        build_runner = True
    elif runner_path.stat().st_mtime < config.runner_source_path.stat().st_mtime:
        logging.info("%s is older than %s: rebuilding", runner_path, config.runner_source_path)
        build_runner = True

    if build_runner:
        runner_build_path = commit_build_path / "runner-build"
        _run([
            "cmake",
            "-G", "Ninja",
            "-S", ".",
            "-B", str(runner_build_path.resolve()),
            f"-DCMAKE_PREFIX_PATH={commit_install_path.resolve()}",
            f"-DCMAKE_INSTALL_PREFIX={commit_install_path.resolve()}",
            f"-DCMAKE_BUILD_TYPE={config.build_type}"
        ])
        _run([
            "cmake",
            "--build", str(runner_build_path.resolve()),
            "--target", "install"
        ])
        logging.info("cleaning up build dir %s", runner_build_path)
        shutil.rmtree(runner_build_path)
        logging.info("built runner at %s", runner_path)

def _build_runners_for(config, commits):
    # The reason this is separate from the general "build opensim" step
    # is because the runners are more likely to change in-tree but are
    # only one source file, so can be parallelized more easily.
    with ThreadPoolExecutor() as executor:
        task_iterator = executor.map(_build_runner_for, [config for _ in commits], commits)
        list(task_iterator)

def _build_commit(config, commit): 
    commit_build_path = config.builds_path / commit
    commit_install_path = config.installs_path / commit
    runner_path = commit_install_path / "bin" / "osimperf2-runner"

    assert os.path.commonpath([commit_build_path.resolve(), commit_install_path.resolve()]) != str(commit_build_path.resolve())

    # Check, configure, build, and install the opensim-core API (if necessary)
    if not commit_install_path.exists():
        # Ensure builds/ directory exists
        config.builds_path.mkdir(parents=True, exist_ok=True)

        # Checkout commit
        logging.info("%s requires building, building it", commit)
        logging.info("checking out %s", commit)
        _run(["git", "-C", str(config.source_path), "checkout", commit])

        # Configure + build dependencies
        dependency_build_path = commit_build_path / "dependencies-build"
        dependency_install_path = commit_build_path / "dependencies-install"
        logging.info("building %s's dependencies to %s", commit, dependency_build_path)
        _run([
            "cmake",
            "-S", str(config.source_path / "dependencies"),
            "-B", str(dependency_build_path.resolve()),
            "-G", "Ninja",
            f"-DCMAKE_INSTALL_PREFIX={dependency_install_path.resolve()}",
            f"-DCMAKE_BUILD_TYPE={config.build_type}",
            "-DOPENSIM_WITH_CASADI=OFF",
            "-DOPENSIM_WITH_TROPTER=OFF",
            "-DSUPERBUILD_docopt=OFF",
            "-DSUPERBUILD_BTK=OFF",
            "-DSUPERBUILD_ipopt=OFF",
            "-DSUPERBUILD_adolc=OFF",
            "-DSUPERBUILD_casadi=OFF",
        ])
        _run([
            "cmake",
            "--build", str(dependency_build_path.resolve())
        ])

        # Configure + build + install OpenSim
        main_build_path = commit_build_path / "main-build"
        logging.info("building %s in %s", commit, main_build_path)
        _run([
            "cmake",
            "-S", str(config.source_path),
            "-B", str(main_build_path.resolve()),
            "-G", "Ninja",
            f"-DCMAKE_INSTALL_PREFIX={commit_install_path.resolve()}",
            "-DBUILD_JAVA_WRAPPING=OFF",
            "-DBUILD_PYTHON_WRAPPING=OFF",
            f"-DCMAKE_BUILD_TYPE={config.build_type}",
            "-DOPENSIM_DISABLE_LOG_FILE=ON",
            "-DOPENSIM_WITH_CASADI=OFF",
            "-DOPENSIM_WITH_TROPTER=OFF",
            "-DOPENSIM_COPY_DEPENDENCIES=ON",
            f"-DOPENSIM_DEPENDENCIES_DIR={dependency_install_path.resolve()}",
            "-DBUILD_API_ONLY=ON",
            "-DBUILD_TESTING=OFF",
            "-DBUILD_API_EXAMPLES=OFF",
            "-DOPENSIM_WITH_MOCO=OFF",
        ])
        _run([
            "cmake",
            "--build", str(main_build_path.resolve()),
            "--target", "install"
        ])

        # Create bin/ install directory (some legacy opensim-core cmake files break otherwise)
        (commit_install_path / "bin").touch()

        # Patch headers that contain stuff that isn't compatible with modern C++
        broken_header = commit_install_path / "include/OpenSim/Common/PropertySet.h"
        if broken_header.exists():
            content = broken_header.read_text()
            patched_content = content.replace(") throw (Exception)", ")")
            if patched_content != content:
                logging.info(f"patching checked exceptions out of {broken_header}")
                broken_header.write_text(patched_content)

        # Clean out build directory (it's installed)
        logging.info("cleaning out %s", commit_build_path)
        shutil.rmtree(commit_build_path)
    else:
        logging.info("%s opensim-core is already built, skipping", commit)

def _all_sample_dates(config):
    end = date.today()
    start = date(end.year - 8, end.month, end.day)
    return _date_sample_grid(start, end)

def all_commits(config):
    for date in _all_sample_dates(config):
        commit = _commit_for(config, date)
        print(commit)

def build_all(config):
    commits = []
    for d in _all_sample_dates(config):
        commit = _commit_for(config, d)
        _build_commit(config, commit)
        commits.append(commit)
    _build_runners_for(config, commits)

def build(config, commit):
    _build_commit(config, commit)
    _build_runners_for(config, [commit])

def measure(config, commit, benchmark_name):
    runner_path = config.installs_path / commit / "bin" / "osimperf2-runner"
    assert runner_path.exists(), f"{runner_path}: does not exist, has `osimperf2-runner` been built for this commit?"

    benchmark_config_path = Path("benchmarks") / benchmark_name / "benchmark_config.toml"
    assert benchmark_config_path.exists()

    runner = _load_runner_from_benchmark_config(benchmark_config_path)
    dt = runner.run(runner_path)
    print(dt)

def full_analysis(config, analysis_toml):
    # Load analysis configuration toml file
    with open(analysis_toml, "rb") as f:
        analysis_config = tomllib.load(f)

    validation_errors = []

    commit_to_date_lut = CommitDateLut()  # accelerates date lookups

    # Collect + validate `commits`
    commits = []
    for commit_config_entry in analysis_config["commits"]:
        for expanded_entry in glob.glob(commit_config_entry):
            commit_hash = Path(expanded_entry).name
            commit_date = commit_to_date_lut.lookup(config, commit_hash)
            commit_runner_path = (Path(expanded_entry) / "bin/osimperf2-runner").resolve()
            if commit_runner_path.exists():
                commits.append(RunnableCommit(commit_date, commit_hash, commit_runner_path))
            else:
                validation_errors.append(f"commits: {commit_runner_path}: does not exist. Maybe you need to run `build-all`?")

    # Collect + validate `benchmarks`
    benchmark_runners = []
    for benchmark_config_entry in analysis_config["benchmarks"]:
        for expanded_entry in glob.glob(benchmark_config_entry):
            benchmark_config_path = Path(expanded_entry) / "benchmark_config.toml"
            if benchmark_config_path.exists():
                benchmark_runners.append(_load_runner_from_benchmark_config(benchmark_config_path))
            else:
                validation_errors.append(f"benchmarks: {benchmark_config_path}: does not exist. Add a benchmark configuration file.")

    # Read + validate `repeats`
    repeats = 1
    if isinstance(analysis_config["repeats"], int) and analysis_config["repeats"] > 0:
        repeats = analysis_config["repeats"]
    else:
        validation_errors.append("repeats: is not a valid configuration value, must be a positive integer")

    logging.info("%s commits, %s benchmarks, and %s repeats will be analyzed (total combinations = %s)", len(commits), len(benchmark_runners), repeats,  len(commits)*len(benchmark_runners)*repeats)

    # Read + validate `result_csv`
    result_csv_path = Path(analysis_config["result_csv"])
    existing_measurements = set()
    if result_csv_path.exists():
        logging.info("%s already exists, so some measurements may be skipped", result_csv_path)
        with open(result_csv_path, newline="", encoding="utf-8") as result_csv:
            reader = csv.DictReader(result_csv)
            for row in reader:
                existing_measurements.add((row["commit"], row["benchmark_name"], row["repeat_number"]))

    # Read + validate `error_csv`
    error_csv_path = Path(analysis_config["error_csv"])
    if error_csv_path.exists():
        logging.info("%s already exists, so some known-to-be-broken measurements may be skipped", error_csv_path)
        with open(error_csv_path, newline="", encoding="utf-8") as error_csv:
            reader = csv.DictReader(error_csv)
            for row in reader:
                existing_measurements.add((row["commit"], row["benchmark_name"], row["repeat_number"]))

    # Exit if there's any validation errors in the analysis configuration
    if validation_errors:
        message = f"Validation errors found in {analysis_toml}:\n"
        for validation_error in validation_errors:
            message += f"  - {validation_error}\n"
        raise RuntimeError(message)

    # Calculate complete sequence of measurements that could be taken
    measurements_to_take : list[AnalysisPlannedMeasurement]  = []
    num_skipped_measurements = 0
    for commit in commits:
        for benchmark_runner in benchmark_runners:
            for repeat in range(0, repeats):
                entry_to_check = (commit.commit, benchmark_runner.name, str(repeat))
                if entry_to_check not in existing_measurements:
                    measurements_to_take.append(AnalysisPlannedMeasurement(commit, benchmark_runner, repeat))
                    logging.debug("add %s", entry_to_check)
                else:
                    num_skipped_measurements += 1
                    logging.debug("skip %s", entry_to_check)

    logging.info("%s measurements will be made (%s already made/errored %s)", len(measurements_to_take), num_skipped_measurements)

    # Shuffle measurements that should be taken
    logging.info("shuffling measurements")
    random.shuffle(measurements_to_take)

    # Write result header (if creating a new result file)
    result_csv_fieldnames = ["measurement_time", "commit", "commit_time", "benchmark_name", "repeat_number", "runtime"]
    if not result_csv_path.exists():
        # Write CSV header
        with open(result_csv_path, "wt") as f:
            writer = csv.DictWriter(f, fieldnames=result_csv_fieldnames)
            writer.writeheader()

    # Write error header (if creating a new error file)
    error_csv_fieldnames = ["commit", "benchmark_name", "repeat_number"]
    if not error_csv_path.exists():
        # Write CSV header
        with open(error_csv_path, "wt") as f:
            writer = csv.DictWriter(f, fieldnames=error_csv_fieldnames)
            writer.writeheader()

    for measurement_to_take in measurements_to_take:
        try:
            measurement = measurement_to_take.run()
        except Exception as ex:
            logging.error(f"error taking measurement: {ex}: appending this error to {error_csv_path}")

            # Append error to error CSV (to prevent re-runs)
            with open(error_csv_path, "a", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=error_csv_fieldnames)
                writer.writerow(measurement_to_take.to_string_dict())
            continue
        measurement_dict = measurement.to_string_dict()

        logging.info("measurement result = %s", measurement_dict)

        # Append measurement to result file
        with open(result_csv_path, "a", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=result_csv_fieldnames)
            writer.writerow(measurement_dict)

def _draw_fig_overlays(events, agg, ax):
    for event in events:
        x0 = event["time"]

        # Y should try to be close to a datapoint
        closest_x = agg.index.get_indexer([x0], method="pad")
        closest_x = agg.index[closest_x][0]
        y0 = agg.loc[closest_x, "mean"]

        annotation_y = y0
        yoffset = 0
        if event.get("yattop"):
            annotation_y = ax.get_ylim()[1]
            yoffset = -9
        if event.get("yatbot"):
            annotation_y = ax.get_ylim()[0]
            yoffset = 2

        # Draw vertical line
        if event.get("drawline"):
            ys = [y0, annotation_y]
            ax.plot([x0, x0], [y0, annotation_y], linestyle="--", linewidth=1, color="lightgrey")
            #ax.axvline(x=x0, linestyle="--", linewidth=1, color="lightgrey")
        # Draw annotation
        ax.annotate(
            event["label"],
            xy=(x0, annotation_y),
            xytext=(1, yoffset),  # offset in points
            textcoords="offset points",
            fontsize=9
        )

def plot_figure1(config : Config, result_csv_path, output_image_path):
    benchmark_name = "RajagopalDropUndamped"

    # These are significant events w.r.t. the profiler-led (i.e. traditional) performance improvements
    significant_events = [
        {"time": pd.to_datetime("2020-05-01"), "label": "Project Start", "yattop": True, "drawline": True},
        {"time": pd.to_datetime("2020-06-23"), "label": "#2806: Cache controllers"},
        {"time": pd.to_datetime("2020-07-23"), "label": "#2837: Optimize cache variables"},
        {"time": pd.to_datetime("2020-12-18"), "label": "#2908: Reduce string construction"},
        {"time": pd.to_datetime("2022-02-03"), "label": "#3125: Tweak wrap cylinder calculations"},
        {"time": pd.to_datetime("2023-05-16"), "label": "#3442: Deduplicate muscle curves"},
        {"time": pd.to_datetime("2024-01-01"), "label": "#3653: Calculate value & derivative"},
        {"time": pd.to_datetime("2024-03-19"), "label": "#3745: (regression)", "yatbot": True, "drawline": True},
        {"time": pd.to_datetime("2024-07-14"), "label": "#3782: ComponentPath Optimization"},
        {"time": pd.to_datetime("2025-10-01"), "label": "Project End", "yattop": True, "drawline": True},
    ]

    # Handle data
    df = pd.read_csv(result_csv_path)
    df = df[df["benchmark_name"] == benchmark_name]                  # filter by benchmark
    df["commit_time"] = pd.to_datetime(df["commit_time"], unit="s")  # parse unix timestamps
    agg = df.groupby("commit_time")["runtime"].agg(["mean", "std"])

    # Create top-level figure
    fig, ax = plt.subplots()
    fig.suptitle("Profiler-Led Code Optimizations Doubled Simulation Performance")

    # Create plot
    ax.errorbar(x=agg.index, y=agg["mean"], yerr=agg["std"], fmt="o")
    ax.set_xlabel("Commit Date on OpenSim-Core")
    ax.set_ylabel("Mean Runtime [seconds]")
    ax.set_title(f"Rajagopal2015.osim, Forward Dynamic Simulation (2 seconds simulation time)", fontsize=9)

    # Draw overlays over the plot
    _draw_fig_overlays(significant_events, agg, ax)

    plt.savefig(output_image_path, dpi=300, bbox_inches="tight")

def plot_figure2(config : Config, result_csv_path, output_image_path):
    original_model_benchmark = "RajagopalDropUndamped"
    fixed_model_benchmark = "RajagopalDrop"

    # These are significant events w.r.t. identifying model bottlenecks
    significant_events = [
        {"time": pd.to_datetime("2020-05-01"), "label": "Project Start", "yattop": True, "drawline": True},
        {"time": pd.to_datetime("2023-10-24"), "label": "Model Bottleneck Identified"},
        {"time": pd.to_datetime("2025-10-01"), "label": "Project End", "yattop": True, "drawline": True},
    ]

    # Handle original data
    df_original = pd.read_csv(result_csv_path)
    df_original = df_original[df_original["benchmark_name"] == original_model_benchmark]  # filter by benchmark
    df_original["commit_time"] = pd.to_datetime(df_original["commit_time"], unit="s")     # parse unix timestamps
    agg_original = df_original.groupby("commit_time")["runtime"].agg(["mean", "std"])

    # Handle fixed (dampened) data
    df_fixed = pd.read_csv(result_csv_path)
    df_fixed = df_fixed[df_fixed["benchmark_name"] == fixed_model_benchmark]  # filter by benchmark
    df_fixed["commit_time"] = pd.to_datetime(df_fixed["commit_time"], unit="s")     # parse unix timestamps
    df_fixed = df_fixed[df_fixed["commit_time"] >= pd.to_datetime("2023-10-24")]
    agg_fixed = df_fixed.groupby("commit_time")["runtime"].agg(["mean", "std"])

    # Create top-level figure
    fig, ax = plt.subplots()
    fig.suptitle("Identifying Model Bottlenecks Tripled Simulation Performance")

    # Create plot
    ax.errorbar(x=agg_original.index, y=agg_original["mean"], yerr=agg_original["std"], fmt="o")
    ax.errorbar(x=agg_fixed.index, y=agg_fixed["mean"], yerr=agg_fixed["std"], fmt="o")
    ax.set_xlabel("Commit Date on OpenSim-Core")
    ax.set_ylabel("Mean Runtime [seconds]")
    ax.set_title(f"Forward Dynamic Simulation (2 seconds simulation time)", fontsize=9)

    _draw_fig_overlays(significant_events, agg_fixed, ax)

    plt.savefig(output_image_path, dpi=300, bbox_inches="tight")

def main():
    config = Config()
    logging.basicConfig(level=config.log_level)

    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest="command", required=True)

    subparsers.add_parser("all-commits")          # no args
    subparsers.add_parser("build-all")            # no args
    
    full_analysis_parser = subparsers.add_parser("full-analysis")
    full_analysis_parser.add_argument("measurements_toml")

    build_parser = subparsers.add_parser("build")
    build_parser.add_argument("commit")

    measure_parser = subparsers.add_parser("measure")
    measure_parser.add_argument("commit")
    measure_parser.add_argument("benchmark_name")

    plot_figure1_parser = subparsers.add_parser("plot-figure1")
    plot_figure1_parser.add_argument("result_csv_path")
    plot_figure1_parser.add_argument("output_image_path")

    plot_figure2_parser = subparsers.add_parser("plot-figure2")
    plot_figure2_parser.add_argument("result_csv_path")
    plot_figure2_parser.add_argument("output_image_path")

    args = parser.parse_args()
    if args.command == "all-commits":
        all_commits(config)
    elif args.command == "build-all":
        build_all(config)
    elif args.command == "full-analysis":
        full_analysis(config, args.measurements_toml)
    elif args.command == "build":
        build(config, args.commit)
    elif args.command == "measure":
        measure(config, args.commit, args.benchmark_name)
    elif args.command == "plot-figure1":
        plot_figure1(config, args.result_csv_path, args.output_image_path)
    elif args.command == "plot-figure2":
        plot_figure2(config, args.result_csv_path, args.output_image_path)
    else:
        raise RuntimeError("unhandled command detected: probably a developer's fault!")

if __name__ == "__main__":
    main()
